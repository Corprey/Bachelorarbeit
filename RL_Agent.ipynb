{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadd2584-c241-46b8-9f4e-b2cb133e403b",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent and Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd122de-298f-4bf0-a2ef-8ab7d4eeae69",
   "metadata": {},
   "source": [
    "## Install and define dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddfd8f73-3368-423f-8dfa-a28cbd03b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "import torch\n",
    "import sys\n",
    "sys.modules[\"gym\"] = gym\n",
    "from stable_baselines3 import SAC\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27a8c18f-27c7-4780-b0a6-46884cc08d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to training images\n",
    "train_image_path = \"../../GTSDB/images/distorted\"\n",
    "original_image_path = \"../../GTSDB/images\"\n",
    "label_path = \"../../GTSDB/labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6c88b-9427-4d0b-9de6-24de6f09edfd",
   "metadata": {},
   "source": [
    "## Build Environment\n",
    "\n",
    "Action: The Agent is able to use continous values to change each image parameter (Sharpness, Brightness, Contrast, Color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56bbf66e-413f-48b2-b2ef-e9095f30d7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\phili/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-6-7 Python-3.10.9 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1050 Ti, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 1817344 parameters, 0 gradients, 4.3 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='./models/YOLOv5_best_1000ep.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca9bb9e8-13e4-481f-b49d-039591aafd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         xmin        ymin        xmax        ymax  confidence  class  \\\n",
      "0  774.867065  409.339874  814.390442  445.731384    0.272725     11   \n",
      "\n",
      "                                    name  \n",
      "0  Right-of-way at the next intersection  \n"
     ]
    }
   ],
   "source": [
    "# Images\n",
    "img = Image.open(\"../../GTSDB/images/00000.png\")\n",
    "\n",
    "# Inference\n",
    "temp = np.asarray(img)\n",
    "results = model(temp)\n",
    "\n",
    "# Results\n",
    "temp2 = results.pandas().xyxy[0]  # Pandas DataFrame\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "759a17ff-c32a-42c8-a68d-af8e79c49f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistortionEnv(Env):\n",
    "\n",
    "    image_width = 640\n",
    "    image_height = 640\n",
    "\n",
    "    \n",
    "    # Resizes the image to 128x128 and converts to 3 color channels\n",
    "    @staticmethod\n",
    "    def _load_and_convert_image(image_path):\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((DistortionEnv.image_width, DistortionEnv.image_height))\n",
    "        if image.mode == \"RGBA\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        return np.asarray(image)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_mse(image1, image2):\n",
    "   \n",
    "        # Resize the images if necessary (to ensure they have the same dimensions)\n",
    "        image1 = image1.resize((DistortionEnv.image_width, DistortionEnv.image_height))\n",
    "        image2 = image2.resize((DistortionEnv.image_width, DistortionEnv.image_height))\n",
    "    \n",
    "        # Convert the images to grayscale\n",
    "        image1 = image1.convert(\"L\")\n",
    "        image2 = image2.convert(\"L\")\n",
    "    \n",
    "        # Convert the images to numpy arrays\n",
    "        arr1 = np.array(image1)\n",
    "        arr2 = np.array(image2)\n",
    "    \n",
    "        # Calculate the MSE\n",
    "        mse = np.mean((arr1 - arr2) ** 2)\n",
    "        return mse\n",
    "\n",
    "    # Convert YOLOv5 format labeled bounding boxes to coordinates\n",
    "    @staticmethod\n",
    "    def _convert_yolo_to_coordinates(yolo_box, image_width, image_height):\n",
    "        net_class, center_x, center_y, width, height = yolo_box\n",
    "        \n",
    "        xmin = (center_x - width/2) * image_width\n",
    "        ymin = (center_y - height/2) * image_height\n",
    "        xmax = (center_x + width/2) * image_width\n",
    "        ymax = (center_y + height/2) * image_height\n",
    "        return net_class, xmin, ymin, xmax, ymax\n",
    "\n",
    "    def _get_gt_coords_from_yolo_file(self, file_path):\n",
    "        yolo_boxes = []\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    line = line.strip()  \n",
    "                    net_class, center_x, center_y, width, height = line.split()     \n",
    "                    \n",
    "                    center_x = float(center_x)\n",
    "                    center_y = float(center_y)\n",
    "                    width = float(width)\n",
    "                    height = float(height)\n",
    "                    \n",
    "                    entry = (net_class, center_x, center_y, width, height)\n",
    "                    yolo_boxes.append(entry)\n",
    "\n",
    "        # Convert Yolo bounding boxes to coordinates\n",
    "        gt_coords = []\n",
    "        for yolo_box in yolo_boxes:\n",
    "            box_coords = self._convert_yolo_to_coordinates(yolo_box, DistortionEnv.image_width, DistortionEnv.image_height)\n",
    "            gt_coords.append(box_coords)\n",
    "        return gt_coords\n",
    "        \n",
    "    @staticmethod\n",
    "    def _calculate_iou(box1, box2):\n",
    "        class1, xmin1, ymin1, xmax1, ymax1 = box1\n",
    "        class2, xmin2, ymin2, xmax2, ymax2 = box2\n",
    "\n",
    "        if (str(class1) != str(class2)):\n",
    "            return 0\n",
    "        \n",
    "        # Calculate intersection area\n",
    "        xmin_inter = max(xmin1, xmin2)\n",
    "        ymin_inter = max(ymin1, ymin2)\n",
    "        xmax_inter = min(xmax1, xmax2)\n",
    "        ymax_inter = min(ymax1, ymax2)\n",
    "    \n",
    "        width_inter = max(0, xmax_inter - xmin_inter)\n",
    "        height_inter = max(0, ymax_inter - ymin_inter)\n",
    "        area_inter = width_inter * height_inter\n",
    "    \n",
    "        # Calculate union area\n",
    "        area_box1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "        area_box2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "        area_union = area_box1 + area_box2 - area_inter\n",
    "    \n",
    "        # Calculate IoU\n",
    "        iou = area_inter / area_union if area_union > 0 else 0\n",
    "        return iou\n",
    "\n",
    "\n",
    "    def _check_ious(self, image_width, image_height, gt_coords, predictions):\n",
    "        # Iterate over prediction rows, calculate IoU with each ground truth box, keep highest IoU\n",
    "        iou_values = []\n",
    "        for index, row in predictions.iterrows():\n",
    "            predicted_box = (row['class'], row['xmin'], row['ymin'], row['xmax'], row['ymax'])\n",
    "        \n",
    "            # Calculate IoU with each ground truth box\n",
    "            iou_per_gt = []\n",
    "            for gt_coord in gt_coords:\n",
    "                iou = self._calculate_iou(gt_coord, predicted_box)\n",
    "                iou_per_gt.append(iou)\n",
    "        \n",
    "            # Store the maximum IoU value for the predicted box\n",
    "            if(len(iou_per_gt) != 0):\n",
    "                max_iou = max(iou_per_gt)\n",
    "                iou_values.append(max_iou)\n",
    "\n",
    "        if (len(iou_values) != 0):\n",
    "            mean_iou = sum(iou_values) / len(iou_values)\n",
    "        else:\n",
    "            if(len(gt_coords) == 0 and len(predictions) == 0):\n",
    "                mean_iou = 1.0\n",
    "            else:\n",
    "                mean_iou = 0.0\n",
    "        \n",
    "        return mean_iou\n",
    "\n",
    "    \n",
    "   # @staticmethod\n",
    "    #def _calculate_F1():\n",
    "    \n",
    "    #@staticmethod\n",
    "#    def _calculate_reward():\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load YOLOv5 model\n",
    "        self.model = torch.hub.load('ultralytics/yolov5', 'custom', path='./models/YOLOv5_best_1000ep.pt')\n",
    "        \n",
    "        # Define the action space bounds for sharpness, contrast, brightness and color\n",
    "        sharpness_bounds = (0.0, 2.0)\n",
    "        contrast_bounds = (0.0, 2.0)\n",
    "        brightness_bounds = (0.0, 2.0)\n",
    "        color_bounds = (0.0, 2.0)\n",
    "        num_actions = 4\n",
    "\n",
    "        # Use when multiple actions are possible\n",
    "        #self.action_space = Box(low = np.array([sharpness_bounds[0], contrast_bounds[0], brightness_bounds[0], color_bounds[0]]),\n",
    "        #                       high = np.array([sharpness_bounds[1], contrast_bounds[1], brightness_bounds[1], color_bounds[1]]),\n",
    "        #                       shape = (num_actions,), \n",
    "        #                       dtype = float)\n",
    "\n",
    "        # TODO: Simplified to only learn adjusting the brightness. Has to be changed later\n",
    "        self.action_space = Box(low = brightness_bounds[0],\n",
    "                               high = brightness_bounds[1],\n",
    "                               shape = (1,), \n",
    "                               dtype = 'float32')\n",
    "\n",
    "        # Define the observation space for an image\n",
    "        image_shape = (DistortionEnv.image_width, DistortionEnv.image_height, 3)  # (height, width, channels)\n",
    "        image_dtype = np.uint8 \n",
    "\n",
    "        # Load training images\n",
    "        self.train_images = os.listdir(train_image_path)\n",
    "\n",
    "        self.observation_space = Box(low = 0, high = 255, shape = image_shape, dtype = np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        # TODO: Set duration? e.g. 10 consecutive actions possible, maybe should start with only 1\n",
    "        self.remaining_actions = 1\n",
    "\n",
    "        # Choose random image for this episode\n",
    "        self.image_name = random.choice(self.train_images)\n",
    "        \n",
    "        # Load input image\n",
    "        input_image_path = os.path.join(train_image_path, self.image_name)\n",
    "        self.image_input = self._load_and_convert_image(input_image_path)\n",
    "        \n",
    "        # Load original image\n",
    "        orig_image_path = os.path.join(original_image_path, self.image_name)\n",
    "        self.image_original = self._load_and_convert_image(orig_image_path)\n",
    "        \n",
    "        # Load ground truth (label and bounding box file)\n",
    "        label_filename = self.image_name.split('.')[0] + \".txt\"\n",
    "        self.gt_coords = self._get_gt_coords_from_yolo_file(os.path.join(label_path, label_filename))\n",
    "\n",
    "        self.state = self.image_input.copy()\n",
    "        return self.state, {}\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.remaining_actions -= 1\n",
    "        \n",
    "        # change image parameters according to action \n",
    "        distortion_factor = action\n",
    "        enhancer = ImageEnhance.Brightness(Image.fromarray(self.state))\n",
    "        enhanced_image = enhancer.enhance(distortion_factor)\n",
    "        self.state = np.asarray(enhanced_image)\n",
    "        \n",
    "        # TODO: Reward calculation other than MSE! only for test purpose\n",
    "        # calculate IoU for each prediction\n",
    "        # test if it works\n",
    "        results = self.model(self.state)\n",
    "        predictions = results.pandas().xyxy[0]\n",
    "        mean_iou = self._check_ious(DistortionEnv.image_width, DistortionEnv.image_height, self.gt_coords, predictions)\n",
    "        \n",
    "        reward = 1 if (mean_iou > 0.5) else -1\n",
    "        observation = self.state\n",
    "        done = True if (self.remaining_actions <= 0) else False\n",
    "        info = {} # Placeholder\n",
    "        \n",
    "        return observation, reward, done, False, info \n",
    "        \n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "063cc2f6-7b4c-4640-a06c-9867541eeea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\phili/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-6-7 Python-3.10.9 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1050 Ti, 4096MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 213 layers, 1817344 parameters, 0 gradients, 4.3 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "env = DistortionEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b6a7ad8-d013-40c4-adc6-d57d7a06a732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:1\n",
      "Episode:2 Score:-1\n",
      "Episode:3 Score:1\n",
      "Episode:4 Score:-1\n",
      "Episode:5 Score:-1\n",
      "Episode:6 Score:1\n",
      "Episode:7 Score:-1\n",
      "Episode:8 Score:1\n",
      "Episode:9 Score:-1\n",
      "Episode:10 Score:1\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, _, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"Episode:{} Score:{}\".format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af3504c6-f3e6-408e-b73e-2e7ac07a8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d74d5178-627e-4412-8d01-76904d49434e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 7            |        cudaMalloc retries: 7         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   2446 MiB |   2453 MiB |   3661 MiB |   1214 MiB |\\n|       from large pool |   2434 MiB |   2435 MiB |   3149 MiB |    715 MiB |\\n|       from small pool |     12 MiB |     29 MiB |    511 MiB |    498 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   2446 MiB |   2453 MiB |   3661 MiB |   1214 MiB |\\n|       from large pool |   2434 MiB |   2435 MiB |   3149 MiB |    715 MiB |\\n|       from small pool |     12 MiB |     29 MiB |    511 MiB |    498 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   2446 MiB |   2453 MiB |   3649 MiB |   1202 MiB |\\n|       from large pool |   2434 MiB |   2435 MiB |   3138 MiB |    704 MiB |\\n|       from small pool |     12 MiB |     28 MiB |    510 MiB |    498 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   2478 MiB |   2504 MiB |   2504 MiB |  26624 KiB |\\n|       from large pool |   2454 MiB |   2474 MiB |   2474 MiB |  20480 KiB |\\n|       from small pool |     24 MiB |     30 MiB |     30 MiB |   6144 KiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  32035 KiB |  32035 KiB |   1685 MiB |   1654 MiB |\\n|       from large pool |  20223 KiB |  28352 KiB |   1078 MiB |   1058 MiB |\\n|       from small pool |  11812 KiB |  11812 KiB |    607 MiB |    595 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     275    |     611    |    5735    |    5460    |\\n|       from large pool |       5    |      11    |     275    |     270    |\\n|       from small pool |     270    |     607    |    5460    |    5190    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     275    |     611    |    5735    |    5460    |\\n|       from large pool |       5    |      11    |     275    |     270    |\\n|       from small pool |     270    |     607    |    5460    |    5190    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      16    |      20    |      20    |       4    |\\n|       from large pool |       4    |       5    |       5    |       1    |\\n|       from small pool |      12    |      15    |      15    |       3    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      86    |      88    |    2495    |    2409    |\\n|       from large pool |       4    |       4    |     103    |      99    |\\n|       from small pool |      82    |      84    |    2392    |    2310    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c364780-ca6a-4892-805f-57dc54416f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.17 GiB (GPU 0; 4.00 GiB total capacity; 2.39 GiB already allocated; 294.45 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSAC\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#model.learn(total_timesteps=1000, log_interval=10)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#model.save(\"sac_pendulum\")\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:148\u001b[0m, in \u001b[0;36mSAC.__init__\u001b[1;34m(self, policy, env, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, action_noise, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, ent_coef, target_update_interval, target_entropy, use_sde, sde_sample_freq, use_sde_at_warmup, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ment_coef_optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:151\u001b[0m, in \u001b[0;36mSAC._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_aliases()\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Running mean and running var\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:200\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer_class(\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size,\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreplay_buffer_kwargs,  \u001b[38;5;66;03m# pytype:disable=wrong-keyword-args\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     )\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_class(  \u001b[38;5;66;03m# pytype:disable=not-instantiable\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space,\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_schedule,\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_kwargs,  \u001b[38;5;66;03m# pytype:disable=not-instantiable\u001b[39;00m\n\u001b[0;32m    199\u001b[0m )\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# Convert train freq parameter to TrainFreq object\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_train_freq()\n",
      "File \u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mD:\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.17 GiB (GPU 0; 4.00 GiB total capacity; 2.39 GiB already allocated; 294.45 MiB free; 2.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = SAC(\"MlpPolicy\", env, verbose=1, buffer_size=1, batch_size=1)\n",
    "#model.learn(total_timesteps=1000, log_interval=10)\n",
    "#model.save(\"sac_pendulum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f97b8-e741-4a73-b5ca-54dcdf10d4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
